{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution for Identifying Issues in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two Step Approach\n",
    "<img src=\"img/data_issue_flow.png\" alt=\"Train & Test Methodology\" width=\"700\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of Approach\n",
    "\n",
    "- __Step1: Heuristic Approach__: validate each column in the data for issues pertaining \"Empty Cells\" and \"Data Type\" with the rules given in \"info-field sheet\". Store indices of rows of columns not meeting the condition.\n",
    "\n",
    "- __Step2: Frequency Based Issue Identification__: All categorical columns post validation from pre defined conditions may still include erroneous values. Such values are likely to have extremly small frequency. __Assuming__ such erroneous values may have frequency < 0.1 then such values can be tagged as __likely__data issues.\n",
    "\n",
    "- __Combining Step1 & 2:__ As an ensemble approach we can consider data issues that were predicted in both the above approaches of either. I have opted to go with selecting index of data issues per column where from approach that identifies less data issues. __Assumption__ : Anomaly or outlier in data is generally the value which is rare in the given population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To the Code......."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load requried packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dateparser\n",
    "import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data\n",
    "- df is the raw data.\n",
    "- metaFile is the sheet containing conditions for validation of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Data_Science_Problem-external.xlsx\",sheet_name=\"Data\")\n",
    "metaFile = pd.read_excel(\"Data_Science_Problem-external.xlsx\",sheet_name= \"field-info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Mapping Table\n",
    "\n",
    "- Includes Regular Expression for validation of special data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mapping table\n",
    "mapping_dict = dict(ip4 = '[0-9]+(?:\\.[0-9]+){3}', Hash = '([a-fA-F\\d]{32})',\n",
    "                   timepStamp = '[0-3][0-9]\\s\\w+\\,\\s[0-9]{4}\\s\\([0-2][0-9]:[0-5][0-9]:[0-5][0-9]\\)',\n",
    "                    alphaNumeric = \"(?=.*[a-zA-Z])(?=.*[@#\\d])[a-zA-Z\\d@#]+\",\n",
    "                   url = '\\w+\\.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1: Heuristic Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_based_data_issues(df, metaFile,mapper):\n",
    "    \n",
    "    df_new = df.dropna(how='all')\n",
    "    # renaming of columns\n",
    "    df_new = df.iloc[:,0:34]\n",
    "    \n",
    "    temp = [] # empty list to hold field ids of issue related fields\n",
    "    for column in df_new.columns:\n",
    "        temp_values = metaFile.loc[metaFile[\"Field Name\"].str.contains(column+\"$\")]\n",
    "        temp_values = temp_values.head(n=1)\n",
    "        #print(temp_values)\n",
    "        # Rules to check the data type\n",
    "        temp_na = []\n",
    "        temp_type = []\n",
    "        # Handle rule for empty cells\n",
    "        if temp_values[\"Can be empty?\"].values == \"N\":\n",
    "            temp_na.extend(df_new[df_new[column].isnull()].index.tolist())\n",
    "        \n",
    "        # Handle rules for data type\n",
    "        if temp_values[\"Type\"].values == \"alpha-numeric\":\n",
    "            alnum = df_new[[column]].apply(lambda x : x.str.contains(mapper[\"alphaNumeric\"]))\n",
    "            temp_type.extend(alnum.index[alnum[column] == False].tolist())\n",
    "#             temp_type.extend(df_new[df_new[column].str.isalnum() == True].index.tolist())  \n",
    "        \n",
    "        elif temp_values[\"Type\"].values == \"char string\":\n",
    "            temp_type.extend(df_new[df_new[column].str.isalnum()== False].index.tolist())\n",
    "        \n",
    "        elif temp_values[\"Type\"].values == \"Integer\":\n",
    "            temp_type.extend(df_new[df_new[column].str.isnumeric()== False].index.tolist())    \n",
    "        \n",
    "        elif temp_values[\"Type\"].values == \"number\":\n",
    "            temp_type.extend(df_new[df_new[column].str.isnumeric()== False].index.tolist())\n",
    "        \n",
    "        elif temp_values[\"Type\"].values == \"string\":\n",
    "            temp_type.extend(df_new[df_new[column].str.isalpha()== False].index.tolist())\n",
    "        \n",
    "        elif temp_values[\"Type\"].values == \"IP v4\":\n",
    "            ip4 = df_new[[column]].apply(lambda x : x.str.contains(mapper[\"ip4\"]))\n",
    "            temp_type.extend(ip4.index[ip4[column] == False].tolist())\n",
    "        \n",
    "        elif temp_values[\"Type\"].values == \"MD5 hash\":\n",
    "            Hash = df_new[[column]].apply(lambda x : x.str.contains(mapper[\"Hash\"]))\n",
    "            temp_type.extend(Hash.index[Hash[column] == False].tolist())\n",
    "        \n",
    "        elif temp_values[\"Type\"].values == \"time stamp\":\n",
    "            timeStamp = df_new[column].apply(lambda x : isinstance(dateparser.parse(str(x)),datetime.datetime))\n",
    "            temp_type.extend(timeStamp.index[timeStamp == False].tolist())\n",
    "        \n",
    "        else:\n",
    "            url = df_new[[column]].apply(lambda x : x.str.contains(mapper[\"url\"]))\n",
    "            temp_type.extend(url.index[url[column] == False].tolist())\n",
    "            \n",
    "            \n",
    "        temp_column_dictionary = {column:{\"Empty_Cell_Issue\":temp_na , \n",
    "                                          \"Type_Issue\":temp_type}}\n",
    "        print(\"Issues Identified for %s..............\"%column)\n",
    "        print(\"Blank cell issues %s\"%(len(temp_na)))\n",
    "        print(\"Data Type issues %s\\n\"%len(temp_type))\n",
    "        #print(temp_type)\n",
    "        temp.append(temp_column_dictionary)\n",
    "        \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues = rule_based_data_issues(df,metaFile,mapping_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: Frequency Based Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_based_issues(df, metaFile,mapper,threshold = 0.1):\n",
    "    df_new = df.dropna(how='all')\n",
    "    # renaming of columns\n",
    "    df_new = df.iloc[:,0:34]\n",
    "    \n",
    "    temp = [] # empty list to hold field ids of issue related fields\n",
    "    exclusded_lables = [\"number\",\"Integer\",\"IP v4\",\"MD5 hash\",\"time stamp\"]\n",
    "    for column in df_new.columns:\n",
    "        temp_values = metaFile.loc[metaFile[\"Field Name\"].str.contains(column+\"$\")]\n",
    "        temp_values = temp_values.head(n=1)\n",
    "        #print(temp_values)\n",
    "        # Rules to check the data type\n",
    "        temp_frequency = []\n",
    "        \n",
    "        # Handle rules for data type\n",
    "        if temp_values[\"Type\"].values not in exclusded_lables:\n",
    "            label_freq = df_new[[column]].apply(lambda x: x.value_counts(normalize = True))\n",
    "            issue_labels = label_freq.index[label_freq[column] < threshold].tolist()\n",
    "            issue_index = [df_new.index[df[column] == x].tolist()[0] for x in issue_labels]\n",
    "            temp_frequency.extend(issue_index)\n",
    "        temp_column_dictionary = {column:{\"freq_Issue\":temp_frequency}}\n",
    "        temp.append(temp_column_dictionary)\n",
    "        \n",
    "        print(\"Issues Identified for %s..............\"%column)\n",
    "        print(\"Frequency based issues %s\\n\"%len(temp_frequency))\n",
    "\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_freq = freq_based_issues(df,metaFile,mapping_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3: Comparing the Above Approaches to Identify final row index for data issues\n",
    "\n",
    "- The method also serializes the identified row indices to a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_issue_extractor(df, metaFile,mapper, json_serialize = True):\n",
    "    \n",
    "    df_new = df.dropna(how='all')\n",
    "    # renaming of columns\n",
    "    df_new = df.iloc[:,0:34]\n",
    "    \n",
    "    temp_frequency = []\n",
    "    exclusded_lables = [\"number\",\"Integer\",\"IP v4\",\"MD5 hash\",\"time stamp\"]\n",
    "    for i,column in enumerate(df_new.columns):\n",
    "        temp_values = metaFile.loc[metaFile[\"Field Name\"].str.contains(column+\"$\")]\n",
    "        temp_values = temp_values.head(n=1)\n",
    "        #print(temp_values)\n",
    "        # Final rule to validate data record anomaly\n",
    "        if len(issues_freq[i][column][\"freq_Issue\"] ) > len(issues[i][column][\"Type_Issue\"]):\n",
    "            final_dict = {column:{\"Empty_Issues_Index\":issues[i][column][\"Empty_Cell_Issue\"],\n",
    "                                 \"Anomalous_Value_Index\":issues_freq[i][column][\"freq_Issue\"]}}\n",
    "\n",
    "        else:\n",
    "            final_dict = {column:{\"Empty_Issues_Index\":issues[i][column][\"Empty_Cell_Issue\"], \n",
    "                                \"Anomalous_Value_Index\":issues[i][column][\"Type_Issue\"]}}\n",
    "        temp_frequency.append(final_dict)\n",
    "    \n",
    "    print(temp_frequency)\n",
    "    \n",
    "    # Seralize object to json file\n",
    "    if json_serialize:\n",
    "        with open('data_issues.json', 'w') as outfile:\n",
    "            json.dump(temp_frequency, outfile)\n",
    "\n",
    "    return temp_frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_issues = data_issue_extractor(df,metaFile,mapping_dict,json_serialize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping up the Exploration...\n",
    "\n",
    "- Identification of issues in data can be enhanced with basic knowledge transfer from SME.\n",
    "- In the light of limited knowledge alongwtih the pre defined rules, statistical method should be incorporated to identify rare values. Rare terms are likely to be outliers and with larger size of data can be effective in identification of issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
